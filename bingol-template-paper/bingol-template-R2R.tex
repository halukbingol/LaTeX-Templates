% !TEX spellcheck = en_US

% ======================================= V
% (NC) 2019 Haluk Bingol
% github.com/halukbingol/LaTeX-Templates
%
% Licensees may copy, distribute, display, and perform the work and make derivative works 
% and remixes based on it only for non-commercial purposes. 
% ======================================= A




\documentclass[10.5pt]{amsart}
%: ==== HB Header Common Packages v20160423 ====V
	\usepackage[utf8]{inputenc} % To use Unicode characters
	\usepackage[iso]{datetime}
	\usepackage{amsmath, amssymb}
	%	\usepackage{etex}
	
	\usepackage[a4paper]{geometry}
	
	\usepackage{xcolor}
	% black, blue, brown, cyan, darkgray, gray, green, lightgray, lime, 
	% magenta, olive, orange, pink, purple, red, teal, violet, white, yellow
	\usepackage[colorlinks=true,linkcolor=red,urlcolor=blue,citecolor=red]%
		{hyperref}
	\usepackage{graphicx,epstopdf}
	% \graphicspath{{fig}}
	\graphicspath{{../common/figures/}}
	% \DeclareGraphicsExtensions{.pdf,.jpeg,.png,.eps}
	% \DeclareGraphicsRule{.tif}{png}{.png}%
	%	{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%	\usepackage{subfigure}
\usepackage{subcaption}



%: ==== HB Header Common Declarations v20160423 ====V
	\newcommand{\hbTimeStamp}{{\color{red}--Draft-- v\today}} % version
	%
	\newcommand{\reffig}[1]{Fig.~\ref{#1}}
	\newcommand{\refeq}[1]{Eq.~\ref{#1}}
	\newcommand{\reftbl}[1]{Table~\ref{#1}}
	\newcommand{\refsec}[1]{Sec.~\ref{#1}}
	\newcommand{\refcite}[1]{Ref~\cite{#1}}
	\newcommand{\hbQuote}[1]{{\small \textsf{``#1''}}}
	%
	\newcommand{\refthm}[1]{Theorem~\ref{#1}}
	\newcommand{\refthmA}[2]{\refthm{#1}(\ref{#2}}
	\newcommand{\reflem}[1]{Lemma~\ref{#1}}
	\newcommand{\refdef}[1]{Definition~\ref{#1}}
	\newcommand{\refexmp}[1]{Example~\ref{#1}}
	%
	\newcommand{\hbFootnote}[2]{\footnote{{\color{red} @#1}#2}}
	\newcommand{\hbIdea}[1]{{\color{olive}{\scriptsize [{#1}]}}}
%	\newcommand{\hbIdea}[1]{} % Empty		
	%	\newcommand{\hbIdea}[1]{}
% ==== HB Header Common Declarations v20160423 ====A

%: ==== HB Header Common for Appendix v20170517 ====V
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{A.\arabic{table}} 
\setcounter{table}{0}
% ==== HB Header Common for Appendix v20170517 ====A

%: ==== HB Header Common for RoR v20170517 ====V
% 	Response to Reviewer
%
\newcommand{\hbRoRCommonPoints}{``Common Points''}
%
\newcommand{\hbColorReviewer}{magenta}
\newcommand{\hbColorManuscript}{violet}
\newcommand{\hbColorManuscriptBackground}{yellow}
%
\usepackage{soul}
\sethlcolor{\hbColorManuscriptBackground}
%
\newenvironment{hbReviewer}
	{\list{}{\leftmargin=2cm\rightmargin=1cm}\item[]``\begin{footnotesize}
	\begin{color}{\hbColorReviewer}}	
	{\end{color}\end{footnotesize}''\endlist}

% connection to the manuscript
%\newcommand{\hbMRef}[1]{``\texttt{{\color{\hbColorManuscript}#1}}''}
\newcommand{\hbMRef}[1]{``\texttt{{\color{\hbColorManuscript}\hl{#1}}}''}
\newcommand{\hbMRefP}[2]{``\texttt{{\color{\hbColorManuscript}\hl{#1}}}''  (page~#2)}
%
%\newenvironment{hbChanged}
%	{\list{}{\leftmargin=0.5cm\rightmargin=1.5cm}\item[]``\begin{small}}	
%	{\end{small}''\endlist}
%
\newenvironment{hbChanged}
	{\list{}{\leftmargin=1cm\rightmargin=1.5cm}\item[]``\begin{small}
	\begin{color}{\hbColorManuscript}}	
	{\end{color}\end{small}''\endlist}
%
\newcommand{\hbMreffig}[1]{\hbMRef{Fig.~#1}}
\newcommand{\hbMreftbl}[1]{\hbMRef{Table.~#1}}

%  ==== HB Header Common for RoR v20170517 ====A

%: ======= reference to the main text ===============V
%% without pace no
%\newcommand{\hbMIntroduction}{\hbMRef{1. Introduction}}
%\newcommand{\hbMMotivation}{\hbMRef{1.1. Motivation}}
%\newcommand{\hbMBackground}{\hbMRef{2. Background}}
%\newcommand{\hbMLanguageModel}{\hbMRef{2.1. Language Model}}
%\newcommand{\hbMComprehension}{\hbMRef{2.1.1. Comprehension}}
%\newcommand{\hbMLearningModel}{\hbMRef{2.1.2. Learning Model}}
%\newcommand{\hbMKMeans}{\hbMRef{2.2. $k$-means Algorithm}}
%\newcommand{\hbMOptimumLanguageClusters}{\hbMRef{2.2.1. Finding Optimum Language Clusters}}
%\newcommand{\hbMModel}{\hbMRef{3. Model}}
%\newcommand{\hbMResultsAndDiscussion}{\hbMRef{4. Results and Discussion}}
%\newcommand{\hbMSubcommunities}{\hbMRef{4.1. Subcommunities}}
%\newcommand{\hbMNumberOfSubcommunities}{\hbMRef{4.2. Number of Subcommunities}}
%\newcommand{\hbMFutureWork}{\hbMRef{4.3. Future Work}}
%\newcommand{\hbMConclusion}{\hbMRef{5. Conclusions}}
%\newcommand{\hbMAppendix}{\hbMRef{Appendix A}}
%\newcommand{\hbMKMeansAlgorithm}{\hbMRef{A.1. $k$-means Algorithm}}
%\newcommand{\hbMKMeansLanguageDomain}{\hbMRef{A.2. $k$-means in Language Domain}}
% with page no
\newcommand{\hbMIntroduction}{\hbMRefP{1. Introduction}{1}}
\newcommand{\hbMMotivation}{\hbMRefP{1.1. Motivation}{1}}
\newcommand{\hbMBackground}{\hbMRefP{2. Background}{2}}
\newcommand{\hbMLanguageModel}{\hbMRefP{2.1. Language Model}{2}}
\newcommand{\hbMComprehension}{\hbMRefP{2.1.1. Comprehension}{2}}
\newcommand{\hbMLearningModel}{\hbMRefP{2.1.2. Learning Model}{3}}
\newcommand{\hbMKMeans}{\hbMRefP{2.2. $k$-means Algorithm}{3}}
\newcommand{\hbMOptimumLanguageClusters}{\hbMRefP{2.2.1. Finding Optimum Language Clusters}{3}}
\newcommand{\hbMModel}{\hbMRefP{3. Model}{4}}
\newcommand{\hbMResultsAndDiscussion}{\hbMRefP{4. Results and Discussion}{5}}
\newcommand{\hbMSubcommunities}{\hbMRefP{4.1. Subcommunities}{5}}
\newcommand{\hbMNumberOfSubcommunities}{\hbMRefP{4.2. Number of Subcommunities}{6}}
\newcommand{\hbMFutureWork}{\hbMRefP{4.3. Future Work}{7}}
\newcommand{\hbMConclusion}{\hbMRefP{5. Conclusions}{7}}
\newcommand{\hbMAppendix}{\hbMRefP{Appendix A}{8}}
\newcommand{\hbMKMeansAlgorithm}{\hbMRefP{A.1. $k$-means Algorithm}{8}}
\newcommand{\hbMKMeansLanguageDomain}{\hbMRefP{A.2. $k$-means in Language Domain}{8}}
%  ======= reference to the main text ===============V


%: ======= specific ===============
\DeclareGraphicsExtensions{.png}
%
\newcommand{\hbAbs}[1]{| #1 |}
\newcommand{\hbNorm}[1]{\left\lVert #1 \right\rVert}
%
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\cov}{cov}
\newcommand{\argmin}[2]{\underset{#1}{\operatorname{arg \, min}}\;#2}
\newcommand{\argmax}[2]{\underset{#1}{\operatorname{arg \, max}}\;#2}
%\newcommand{\hbAvg}[1]{\widehat{#1}}
\newcommand{\hbOpt}[1]{{#1}^*}
\newcommand{\hbMat}[1]{\mathbf{#1}}
\newcommand{\hbSet}[1]{\mathcal{#1}}
%\newcommand{\hbPartition}{[1]}{\mathbb{#1}}
%\newcommand{\hbIndSet}[2]{\mathcal{#1}_{#2}}
%\newcommand{\hbLS}[2]{F( #1 , #2 )}
\newcommand{\hbLS}[2]{F( #1 \leftrightarrow #2 )}
\newcommand{\hbDLS}[2]{F( #1 \to #2 )}
\newcommand{\hbPayoff}[1]{F_{#1}}
\newcommand{\hbInCLS}[1]{W(#1)}
\newcommand{\hbInCLSrandom}[1]{W_{\text{r}}(#1)}
\newcommand{\hbOptInCLS}[1]{\hbOpt{W}(#1)}
\newcommand{\hbDIntCLS}[2]{I({#1 \to #2})}
%\newcommand{\hbIntCLS}[2]{I(#1, #2)}
\newcommand{\hbIntCLS}[2]{I(#1 \leftrightarrow #2)}
\newcommand{\hbPartition}[1]{\mathbb{#1}} % partition P
\newcommand{\hbPartitionBest}[1]{\overline{\hbPartition{P}_{#1}}} % best partition Pwith K-blocks
%\newcommand{\hbPartitionBest}[1]{\hbPartition{P}_{#1}^{*}} % best partition Pwith K-blocks
\newcommand{\hbMBase}{Base Model}
\newcommand{\hbMA}{Model-A}
\newcommand{\hbMB}{Model-B}
\newcommand{\hbMAB}{Models~A and B}
\newcommand{\hbMC}{Model-C}

\newcommand{\hbIC}[2]{I( #1 \leftrightarrow #2 )}

% ======= specific ===============




\begin{document}
\title[Parent Oriented Teacher Selection Causes Language Diversity]{
	Response to Reviewers\\
	~\\
	~\\
	{\footnotesize
		``Parent Oriented Teacher Selection Causes Language Diversity''\\
		version Dated Feb 20, 2017
	}
}
\author{Ibrahim Cimentepe}
\author{Haluk O. Bingol}
%\affiliation{
%	Department of Computer Engineering\\
%	Bogazici University, Istanbul \\
%	%\today 
%}

\date{\today}
%\date{\hbTimeStamp}


\maketitle




% ======================
\section{Introduction}

We would like to thank our anonymous reviewers 
for pointing out corrections, suggesting ideas and references.
They were very useful and help us to improve readability of the manuscript.




% =======================================
\subsection{Organization}

This response to reviewer is organized as follows.
Individual points are listed under the corresponding reviewers.
Common points that are raised by multiple reviewers 
are addressed in  \refsec{sec:CommonPoints} \hbRoRCommonPoints.




% =======================================
\subsection{Markup in the manuscript}

Changes in the manuscript are highlighted and marked with section number of points.
For example,
changes related to the point
``How $k$-means adapted to language'', 
which is in section 2.2.1,
is marked with 2.2.1.




% =======================================
\subsection{Conventions}

\newcommand{\hbMSample}{\hbMRef{Introduction}}

In order to distinguish 
%
(i)~comments of the reviewers, 
%
(ii)~responses given to the comments, 
%
(iii)~references to the manuscript, and 
%
(iv)~changes done in the manuscript, 
some format, font and color combinations are used.

\begin{hbReviewer}
	This is a sample of comments of reviewer.
	Quotes of the comments of reviewer are given in this format.
\end{hbReviewer}

This is a sample response to the points raised by the reviewer.
During the response, 
one needs to refer to a figure in the manuscript by 
\hbMreffig{3a}.
A section title in the manuscript can be given as \hbMSample.

\begin{hbChanged}
	This is a sample of changes done.
	If one needs to quote the changes done in the manuscript,
	they are given in this format.
\end{hbChanged}





% ======================
\section{Report to the Reviewer \#1}




% =======================================
\subsection{General}
\begin{hbReviewer}
	The authors consider an existing model that describes how language could
	evolve when transferred from one generation to another and extended it
	to the setting in which each offspring learns their language from a particular
	individual who is either geographically or language-wise close to the parent
	of that offspring. They find that such an extended model can promote
	emergence of several subcommunities that share a language, as opposed to
	a global community sharing a universal language.
	
	I like the idea of the work. Such an extension seems natural and the findings
	are interesting. The paper is well structured and once the reader gets used to
	omnipresent typos it reads quite smoothly. I have three major comments
	and several minor suggestions that I list below (page X,line Y):
\end{hbReviewer}

Thanks.




% =======================================
\subsection{Major Points}




% =======================================
\subsubsection{How $k$-means adapted to language}
\begin{hbReviewer}
	p3,l74:
	I very much like the idea of using the k-means clustering algorithm but
	I'd like to see in more detail how it is adapted to the language domain 
	(possibly in an Appendix).
	For example, as far as I know, in the traditional k-means algorithm,
	at some point one takes the ``centers'' of each current cluster.
	I don't see how this is done in the adaptation 
	which renders the findings of the paper irreproducible. 
\end{hbReviewer}

We agree that our $k$-means adaptation to language domain 
is not given properly.
As suggested, an \hbMAppendix\ is added. 
First $k$-means description in $\mathbb{R}^{D}$ is given.
Then adaptation of $k$-means to language domain is described. 
How to handle ``centers''  in the domain of language is disguised in detail.




% =======================================
\subsubsection{Clusters vs language communities}
\label{sec:ClustersLanguageCommunities}
\begin{hbReviewer}
	p4,l11:
	While I agree that defining average within community comprehension and
	taking K that maximizes it is the most natural thing to do, I'd like to see some
	discussion of why such clusters should correspond to language communities 
	 (as in p7,l25).
	For example, in principle it might happen that 
	 the people with worst mutual understanding
	are all lumped together into one cluster, even though, logically, 
	 they are not a single
	language community.
	(This might be checkable by looking at minimum 
	 within community comprehension.)
\end{hbReviewer}

Thanks for pointing this important issue.
%By definition, $k$-means algorithm is forced to organize $K$ clusters.
%If the nature of the data set is not agree with the selected value of $K$,
%the clusters would not make sense.
%%It is very possible that $k$-means points clusters 
%%that are not well suited to language communities.
%That is, 
%it may group agents that do not ``speak'' the same language in a cluster.
%
%This issue is addressed in different sections.
%
The concept of inter-community comprehension is introduced in
section \hbMOptimumLanguageClusters\ and 
\hbMreffig{2b} is given and  discussed in \hbMSubcommunities.\\
Please see \refsec{sec:diversity} for details.

Potential problems of $k$-means algorithm are discussed in the \hbMAppendix.

%One potential source of problem, 
%that comes with $k$-means algorithm,
%is the selection of $K$ not suitable to the data set.
%Our adaptation carries the same weaknesses.
%This is addressed in different sections.
%(i)~Section \hbMOptimumLanguageClusters\ discusses 
%finding the ``best'' $K$ for the data set and 
%why clusters are expected to be language communities.
%%
%(ii)~In the section on \hbMFutureWork, 
%some discussion on this problem is added, too.
%%
%(iii)~Potential problems of $k$-means algorithm are discussed in the \hbMAppendix.  





% =======================================
\subsubsection{``KË†* is independent of .. N''}
\begin{hbReviewer}
	p6,l58:
	I don't quite agree with the (bold) statement  "K\^{}* is independent of .. N".
	In Figure 3, the red data points (N=50) seem to be mostly above the green ones (N=100),
	which seem to be in turn mostly above the blue and purple ones (N=150,200).
	To me, that seems like a dependence on N, although a weak one.
	I'd suggest putting the statement more mildly.
\end{hbReviewer}

%\newcommand{\hbMAB}{Models~A and B}
%\newcommand{\hbOpt}[1]{{#1}^*}


We agree that the statement
``We conclude that for \hbMAB,
the optimum community count $\hbOpt{K}$ is 
independent of the size of the population $N$'' 
is difficult to justify. 
The discussion of \hbMreffig{3} in \hbMNumberOfSubcommunities\ is revised  accordingly.
Now it reads:
\begin{hbChanged}
	(ii)~There is a weak relation between $K^{*}$ and $N$.
	For each model, 
	the value of $K^{*}$ is slightly less for larger values of $N$.
\end{hbChanged}





% =======================================
\subsubsection{Gamma values}
	\label{sec:AGammaValues}
\begin{hbReviewer}
	On a related note, what is gamma, empirically, for N=50, 100, 150, 200?
\end{hbReviewer}

For $\gamma$ values, please see \refsec{sec:R2PowerLaw} in \hbRoRCommonPoints.





% =======================================
\subsection{Minor Points}





% =======================================
\subsubsection{p3,l12:}
\begin{hbReviewer}
	p3,l12:
	I suggest "Language evolves \_both\_ through .. \_and\_ as .."
\end{hbReviewer}

Thanks for the correction. It is done.
Now, in \hbMLearningModel, it reads:
\begin{hbChanged}
	Language evolves
	both through agents interacting with each other within a  generation, 
	and as it is transferred from one generation to the next by means of learning.
\end{hbChanged}




% =======================================
\subsubsection{p4,l5:}
\begin{hbReviewer}
	p4,l5:
	This sentence might be misleading. 
	I believe the selected partition is not necessarily the optimal one but 
	the one found by the algorithm 
	(which is, I agree, probably reasonably close to the optimal one). 
	I suggest putting the sentence more mildly.
\end{hbReviewer}

It is true that $k$-means may not provide the optimum partition but near optimum is expected. 
The wording in \hbMOptimumLanguageClusters\
is changed accordingly.
Now it reads:
\begin{hbChanged}
	For a given $K$,
	$k$-means algorithm provides partition
	$\hbPartitionBest{K}$,
	which is expected to be close to the partition with
	the maximum 
	average within community comprehension.
\end{hbChanged}




% =======================================
\subsubsection{p5,l4:}
\begin{hbReviewer}
	p5,l4:
	Parameters M, S, Q are set to some specific values. 
	Is there anything to be said about the dependence on them?
\end{hbReviewer}

In principle parameters $M$, $S$, $Q$ are independent and
can be selected in any way.
For example, Ref~\cite{nowak1999JTB} uses $M = S = 5$ and $Q = 1, 4, 7, 10$.
%Ref~\cite{nowak2000evolution} uses $M = S = 4$ and $M = S = $
Ref~\cite{nowak1999PNAS} uses $M = 20 < S = 40$.

We prefer to report the case, 
where there are more symbols than meanings.
i.e. $M < S$ .
We used the values $M = 8 < S = 15$ and $Q = 4$.
We also investigate $(5,5), (5,8), (8,5)$ and $(8,8)$ values for $(M, S)$.
Different sampling sizes $Q = 1, 4, 7, 10$ are tested.
We found that the quality of language is low for Q = 1, 
whereas it gets higher and 
stays approximately the same for Q = 4, 7 and 10. 
Therefore Q = 4 seems to be the reasonable selection in our simulation.
As previously reported, 
$Q = 1$ slows down learning since it is too little sampling to learn from teacher.

Since there is not much to say, no change is made in the manuscript.




% =======================================
\subsubsection{p5,l5:}
\begin{hbReviewer}
	p5,l5:
	What are the association matrices of the first generation?
\end{hbReviewer}

You are right that this points is missing. 
\hbMModel\ section is changed accordingly.
Now it reads:
\begin{hbChanged}
	The system starts with the first generation,
	whose  association matrices are randomly filled.
	Remaining generations fill their association matrices by learning.
	Every agent $i$ makes exactly one child $i'$.
	The association matrix of a child is initially empty.
	Each child learns her language from her \emph{teacher},
	donated by $t_{i}$.
\end{hbChanged}





% =======================================
\subsubsection{p6,l51:}
\begin{hbReviewer}
	p6,l51: 
	The word "implies" sounds too strong to me, I'd consider "suggests" or 
	the elsewhere frequent "indicates".
\end{hbReviewer}

Thanks for the correction.
``Implies'' is replaced by ``suggests'' in \hbMNumberOfSubcommunities.
Now it reads:
	\begin{hbChanged}
	The shape of the curve in 
	Fig.~2c %\reffig{fig:FigKStar100}
	suggests a power law relation between $\hbOpt{K}$ and $r$, 
	that is,
\end{hbChanged}




% =======================================
\subsubsection{Highlights:}
\begin{hbReviewer}
	I suggest 
	"ones talk" $->$ "someone who talks", 
	"ones that are" $->$ "someone who is", 
	"effects" $->$ "affects"
\end{hbReviewer}

Thanks for the corrections. They are done.




%% ======================
%\newpage



% ======================
\section{Report to the Reviewer \#2}




%================================================
\subsection{General}
\begin{hbReviewer}
	This work extends a previous mathematical model 
	to study language learning from parents to offspring, 
	and studies two types of strategies to select teachers to communicate. 
	It is shown that the two types of strategies induce language diversity 
	after iterated learning across generations of learners. 
	It also finds a power-law relation between the optimal comprehension level 
	and the size of imitation set for selecting teachers. 
	The mathematical parts of the model are solid, 
	and the results are convincing (though see below). 
	It is a topic suitable for JTB. 
	However, there are several concerns 
	that need to be addressed before this simple work can be published. 
\end{hbReviewer}

Thanks.





% =======================================
\subsection{Gong \& Shuai 2012 Proceeding B}
\begin{hbReviewer}
	First, in the background part, 
	the authors only reviewed previous mathematical models 
	from Nowak's group 
	and briefly mentioned some behavioral models of iterated learning 
	and similar laboratory experiments. 
	However, the idea of learning to people 
	that have high comprehension score during cross-generational learning 
	has been implemented in some behavioral models, 
	such as Gong \& Shuai 2012 Proceeding B. 
	In that work, cultural transmission (selection) is based on 
	individuals' linguistic understandability. 
	This work should be mentioned, 
	since the idea of the current paper is not new. 
\end{hbReviewer}

Thank for pointing out this missing reference.
\hbMIntroduction\ is revised accordingly.
Now it reads:
\begin{hbChanged}
	Yet another group consider language 
	as a means to transfer meanings between individuals
	through signaling structures~[8-10].
	%~\cite{%
	%	nowak2002computational,
	%	gong2012coevolution,
	%	kirby2007innateness}.''
\end{hbChanged}
where Ref~[9] is Gong and Shuai 2012.




% =======================================
\subsection{Comprehension scores of both models are already above 0.125}
\begin{hbReviewer}
	Second, in the result part, there are some confusing points. 
	For example, on page 5, above "Subcommunities", the authors said: 
	"In Fig. 1, comprehensions of both Models A and B are
	 below the threshold of $W_{r}(\mathcal{P}) = 0.125$ for $r < 0.05$". 
	However, it is quite clear in that Fig. 1, 
	except under a small $r < 0.05$, 
	comprehension scores of both models are already above 0.125. 
	Please specify this.
\end{hbReviewer}

We agree that wording was not good.
It is changed accordingly in \hbMResultsAndDiscussion.
Now it reads:
\begin{hbChanged}
	In 
	Fig.~1, %\reffig{fig:FigWP.eps},
	comprehensions of both \hbMAB\ are below 
	the threshold of $\hbInCLSrandom{\hbSet{P}} = 0.125$ 
	for $r < 0.05$.
	The comprehensions increase slowly and reach to the level of \hbMC\ 
	as $r$ approaches to $0.5$.
\end{hbChanged}



% =======================================
\subsection{Diversity}
\label{sec:diversity}
\begin{hbReviewer}
	In addition, since the authors talk about "diversity", 
	only showing within community comprehension score is insufficient, 
	you need to also show comprehension score between communities, 
	in this way, people can really have a sense that the system is diverge! 
	Showing the average comprehension score of all communities is also 
	not enough, 
	because it is possible that the low average score is simply 
	due to low score of some community, 
	and between communities, comprehension score is still high; 
	in other words, every community is trying to develop the common language, 
	but some communities are delayed, 
	so the average score of all communities is not high. 
	This is certainly not divergent! 
\end{hbReviewer}

%This was the most challenging point.
We agree that our presentation was focused on within community comprehension.
We have revised the material and 
included discussion of inter-community comprehension, too.
Inter-community comprehension 
$\hbIntCLS{\hbSet{C}_{\alpha}}{\hbSet{C}_{\beta}}$ 
and 
average inter-community comprehension 
$I({\hbPartition{P}_{K}})$
are defined in \hbMOptimumLanguageClusters.
As suggested, 
average inter-community comprehension are given in \hbMreffig{2b} and discussed in \hbMSubcommunities.

Please also see \refsec{sec:ClustersLanguageCommunities}.






% =======================================
\subsection{Single teacher}
\label{sec:singleTeacher}
\begin{hbReviewer}
	Furthermore, the setting that each child only learns from a single parent 
	throughout the learning process is not realistic at all. 
	Teachers should be chosen proportionally 
	and there must be more than one learning cycle. 
	This actually introduces some noise, 
	because in this process, some individual from the imitation set 
	having low comprehension score can still be chosen, 
	which may delay or even reverse the divergence process. 
	Since most parts of the model are already based on probabilities, 
	the learning part should also be like this, 
	simply by setting more than one learning cycle for each child 
	and let teachers in those cycles be chosen probabilistically. 
\end{hbReviewer}

We agree that having single teacher is too much abstraction but an effective one.
Effects of having many teachers can be investigated as 
it is stated in the \hbMFutureWork\ section.
But we feel that it is not in the scope of this paper.
Now it reads:
\begin{hbChanged}
	For example,
	evolution of language can be perceived as a cultural process
	where some group of people is responsible for the transfer.
	That is, more than one teacher could be assigned to each child.
\end{hbChanged}




% =======================================
\subsection{Power law}
	\label{sec:BPowerLaw}

\begin{hbReviewer}
	Finally, in order to show that the relation between K and r is power-law, 
	why not draw Fig. 3 in a log-log scale to see 
	if there are straight lines or not. In addition, 
	some more discussion on this point is needed. 
\end{hbReviewer}

Please see \refsec{sec:R2PowerLaw} in \hbRoRCommonPoints.




% =======================================
\subsection{Cross-generational cultural transmission}
\begin{hbReviewer}
	Third, in the future work, 
	it would be interesting to further study 
	if grandparent grandchild interactions can help keep language 
	from divergence. 
	This can be simply implemented by keeping the grandparent's status 
	and let child have a chance to learning from them, 
	following exactly the same setting for him/her to learn 
	from parent generation. 
	Some early work like 
	Gong \& Shuai 2015 book chapter: 
	Simulating the effects of cross-generational cultural transmission 
	on language change 
	has addressed this based on a behavioral model. 
	A mathematical model like the one in the current paper 
	may complement the findings. 
\end{hbReviewer}

We agree that,
although it is interesting,
we do not consider the impact of grandparents and their generation in this work.
Probably one should consider this together with 
multi-teacher case of \refsec{sec:singleTeacher} as a future study.
We update the \hbMFutureWork\ section. 
It states the possibility of teachers from the generation of grandparents.
Now it reads:
\begin{hbChanged}
	Once many teachers case is considered,
	one may also consider teachers not only from the parent's generation but 
	the generation of grandparents, too~[20].
%	~\cite{%
%		gong2016CrossGenerational}.
\end{hbChanged}
where Ref~[20] is Gong and Shuai 2015.




% =======================================
\subsection{Some tiny points:}
\begin{hbReviewer}
Some tiny points:

1) page 7, right column, line 6: "effect" should be "affect"

2) page 7, right column, line 14: "which we communicate" should be "whom we communicate"
\end{hbReviewer}

Thanks for the corrections. They are done.




% ======================
\section{Common Points}
	\label{sec:CommonPoints}


Common points that are raised by both reviewers are addressed here. 




% =======================================
\subsection{Power law discussion}
\label{sec:R2PowerLaw}

Issues related to power law are raised in 
\refsec{sec:AGammaValues} 
and 
\refsec{sec:BPowerLaw}. 

Section \hbMNumberOfSubcommunities\ in the manuscript is modified to enhance power law discussion. 
Now \hbMreffig{3}\ has the same values in both linear-linear and log-log scale.
Discussion of straight-line patterns in log-log plot is included 
in \hbMNumberOfSubcommunities.
$\gamma$ values of Model-A and Model-B for various $N$ 
can be seen in \reffig{fig:Gamma}.
These values are given as \hbMreftbl{1} in the manuscript.

% +++++++++++++++++++++++++++++++++++++++
%:-fig gamma
\begin{figure}[!thbp]
	\centering 
	\includegraphics[width=\columnwidth]%
		{Fig-IStar}
	\caption{
		$\gamma$ values of Model-A and Model-B for various $N$.
	} % \caption
	\label{fig:Gamma}
\end{figure}
% +++++++++++++++++++++++++++++++++++++++



% ========================================
\bibliographystyle{ieeetr}
\bibliography{bingol-template-paper}



\end{document}  